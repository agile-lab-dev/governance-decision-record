![new](https://img.shields.io/badge/STATUS-NEW-green)

# Data Product Output Port File

## Context

One of the accepted output port types is "FILE".

## Decision

The following specification apply in our company's context:

- STORAGE SPEC:
  - container type: Storage account with Azure Data Lake storage gen2
  - region: `us-east`
  - encryption: yes, always mandatory
  - public access: none
  - performance: `premium`
  - access tier: `hot`
  - redundancy: `GRS | ZRS`
  - tags: `<domain>`, `<subdomain>` (if any), `analytical`, `<environment>`
  - hierarchical: true, always mandatory
- STORAGE STRUCTURE:
  - different storage accounts for different domains, subdomains, environments, usage (internal or output port). Storage Account name should be `analytical_<domain-subdomain>_<environment>`
  - different containers for different data products. Container name should be `<data-product-name>`.
  - different directories for different **major** versions (assuming Semantic Versioning is leveraged) and different output ports of the data product
- FORMAT:
  - serialization: Parquet
  - compression codec: Snappy
- DATA CONTRACT should include:
  - schema
  - bi-temporality business time reference
  - bi-temporality technical time reference

An example of the [Data Product Specification](https://github.com/agile-lab-dev/Data-Product-Specification/blob/main/example.yaml), in particular focused on the Output Port compoment's metadata, is:

```yaml
id: <data product id>
version: 1.0.0
[...]
components:
- id: urn:dmb:cmp:my_domain:my_data_product:1:top_10_parquet_adlsgen2_port
  name: top-10-historical
  fullyQualifiedName: top 10 historical as parquet file ADLSgen2 output port
  description: an aggregation of top 10 x y for the historical series of my_data_product
  kind: outputport
  version: 1.0.1
  infrastructureTemplateId: output-port-adlsgen2-v1-0-1
  useCaseTemplateId: template-id-1
  dependsOn: []
  platform: Azure
  technology: ADLSgen2
  outputPortType: Files
  processDescription: this output port is generated by a Spark Job scheduled every day at 2AM and it lasts for approx 2 hours
  dataContract:
    schema:
      - name: customerId
        dataType: string
      - name: name
        dataType: string
      - name: surname
        dataType: string
      - name: businessTs
        dataType: timestamp
      - name: writeTs
        dataType: timestamp
      - name: year
        dataType: int
    SLA:
      intervalOfChange: 1 hours
      timeliness: 1 minutes
      upTime: 99.9%
    termsAndConditions: only usable in development environment
    endpoint: https://myurl/development/my_domain/my_data_product/1/my_parquet_file_on_adlsgen2_port
    biTempBusinessTs: businessTs
    biTempTechTs: writeTs
  dataSharingAgreements:
    purpose: this output port want to provide a rich set of profitability KPIs related to the customer
    billing: 5$ for every full scan
    security: In order to consume this output port an additional security check with compliance must be done
    intendedUsage: the dataset is huge so it is recommended to extract maximum 1 year of data and to use these KPIs in the marketing or sales domain, but not for customer care. Data is partitioned by "year", so it's warmly suggested to filter by this field.
    limitations: is not possible to use this data without a compliance check
    lifeCycle: the maximum retention is 10 years, and eviction is happening on the first of january
    confidentiality: if you want to store this data somewhere else, PII columns must be masked    
  tags:
    - tagFQN: mydomain
      source: Tag
      labelType: Manual
      state: Confirmed
    - tagFQN: analytical
      source: Tag
      labelType: Manual
      state: Confirmed
    - tagFQN: svil
      source: Tag
      labelType: Manual
      state: Confirmed
  sampleData: roberto,coluccio,2022-02-10T02:35:00Z,2022-07-30T22:10:00Z
  semanticLinking: {
                     customerId -> Marketing.CRM.websiteLeads.Customer.Id
  }
  specific:
      subscription: 0f876e36-124c-77f1-aabb-e543b3d2b3ad
      resourceGroup: my-resource-group-svil
      storageAccount: analytical-myDomain-svil
      container: top-10
      directory: /1
      geoReplication: GRS
      accessTier: hot
      performance: premium
      region: us-east-1
      serializationFormat: Parquet
      compressionCodec: Snappy
    
 ```


The following changes are considered _BREAKING_:
- schema change in case no out-of-the-box backward compatibility is guaranteed
- bi-temporality fields changes and in general data contract
- serialization format
- compression codec

In the case of a breaking change, the **major** version of the Data Product should change (e.g. 1.x.y -> 2.0.0)

All the other changes are considered _NOT BREAKING_ and should lead to minor or patch Data Product's version change.

## Consequences and accepted trade-offs

Domains must comply to this serialization format and compression codec. In case of major version change, data product owners should inform all the consumers, documenting the changelog and migration plan (including the time period during which both old and new versions are kept in parallel so to avoid breaking changes at the consumers side). Obviously, this implies higher maintenance costs for the DP owner.

## Implementation Steward

The Data Product Owner.
The Platform Team.

## Where the policy becomes computational

- **LOCAL POLICY**: the Data Product Owner should clone and reuse the template for the FILE output port on ADLSgen2 
- **GLOBAL POLICY**: the Platform Team should implement an automated validation of the data product specification in the section specific to the output port's metadata at deploy-time (while requesting the infrastructure provisioning in self-service fashion), so to detect non-compliant requirements along with breaking changes.
